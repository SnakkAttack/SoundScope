{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SoundScope Milestone 1 Exploration\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook documents the first milestone for the SoundScope project. It inspects a prototype dataset of Spotify-style audio features and trains a lightweight baseline model from scratch to classify songs by genre. The goal is to ensure the data pipeline is wired up and to gather initial intuition about how the audio descriptors relate to musical characteristics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "import math\n",
        "import random\n",
        "from collections import Counter, defaultdict\n",
        "from statistics import mean, median, pstdev\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "DATA_PATH = '../data/spotify_tracks.csv'\n",
        "NUMERIC_FEATURES = ['tempo', 'danceability', 'energy', 'valence', 'loudness', 'acousticness', 'instrumentalness', 'speechiness', 'liveness', 'popularity']\n",
        "CATEGORICAL_FIELDS = ['genre', 'mood', 'era']\n",
        "\n",
        "def read_dataset(path):\n",
        "    with open(path, newline='', encoding='utf-8') as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        fieldnames = reader.fieldnames\n",
        "        rows = []\n",
        "        for row in reader:\n",
        "            parsed = {}\n",
        "            for key, value in row.items():\n",
        "                if key in NUMERIC_FEATURES:\n",
        "                    parsed[key] = float(value)\n",
        "                else:\n",
        "                    parsed[key] = value\n",
        "            rows.append(parsed)\n",
        "    return rows, fieldnames\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load prototype dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 25 tracks with 16 columns.\n",
            "Columns: ['track_id', 'track_name', 'artist', 'tempo', 'danceability', 'energy', 'valence', 'loudness', 'acousticness', 'instrumentalness', 'speechiness', 'liveness', 'popularity', 'genre', 'mood', 'era']\n",
            "\n",
            "Preview of the first three tracks:\n",
            "{'track_name': 'Neon Skies', 'artist': 'Lumina', 'genre': 'Pop', 'mood': 'Happy', 'era': '2010s', 'tempo': 120.0, 'danceability': 0.72, 'energy': 0.65}\n",
            "{'track_name': 'Midnight Run', 'artist': 'The Chromatics', 'genre': 'Pop', 'mood': 'Energetic', 'era': '2010s', 'tempo': 128.0, 'danceability': 0.63, 'energy': 0.79}\n",
            "{'track_name': 'Electric Veins', 'artist': 'SynthPulse', 'genre': 'Electronic', 'mood': 'Energetic', 'era': '2020s', 'tempo': 132.0, 'danceability': 0.75, 'energy': 0.88}\n"
          ]
        }
      ],
      "source": [
        "dataset, columns = read_dataset(DATA_PATH)\n",
        "print(f\"Loaded {len(dataset)} tracks with {len(columns)} columns.\")\n",
        "print(\"Columns:\", columns)\n",
        "\n",
        "print(\"\\nPreview of the first three tracks:\")\n",
        "for entry in dataset[:3]:\n",
        "    preview = {\n",
        "        'track_name': entry['track_name'],\n",
        "        'artist': entry['artist'],\n",
        "        'genre': entry['genre'],\n",
        "        'mood': entry['mood'],\n",
        "        'era': entry['era'],\n",
        "        'tempo': entry['tempo'],\n",
        "        'danceability': entry['danceability'],\n",
        "        'energy': entry['energy'],\n",
        "    }\n",
        "    print(preview)\n",
        "\n",
        "numeric_data = {feature: [row[feature] for row in dataset] for feature in NUMERIC_FEATURES}\n",
        "categorical_data = {field: [row[field] for row in dataset] for field in CATEGORICAL_FIELDS}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Dataset overview\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Numeric feature summary (across 25 tracks):\n",
            "- tempo           mean=113.20 | median=118.00 | min= 84.00 | max=150.00 | std= 18.39\n",
            "- danceability    mean=  0.62 | median=  0.63 | min=  0.40 | max=  0.80 | std=  0.12\n",
            "- energy          mean=  0.64 | median=  0.65 | min=  0.35 | max=  0.91 | std=  0.18\n",
            "- valence         mean=  0.54 | median=  0.55 | min=  0.30 | max=  0.75 | std=  0.13\n",
            "- loudness        mean= -6.46 | median= -6.00 | min=-10.00 | max= -3.50 | std=  1.94\n",
            "- acousticness    mean=  0.29 | median=  0.22 | min=  0.05 | max=  0.70 | std=  0.19\n",
            "- instrumentalness mean=  0.16 | median=  0.15 | min=  0.01 | max=  0.45 | std=  0.13\n",
            "- speechiness     mean=  0.05 | median=  0.05 | min=  0.03 | max=  0.08 | std=  0.01\n",
            "- liveness        mean=  0.16 | median=  0.15 | min=  0.09 | max=  0.27 | std=  0.05\n",
            "- popularity      mean= 62.32 | median= 65.00 | min= 42.00 | max= 76.00 | std=  9.84\n"
          ]
        }
      ],
      "source": [
        "print(\"Numeric feature summary (across 25 tracks):\")\n",
        "for feature in NUMERIC_FEATURES:\n",
        "    values = numeric_data[feature]\n",
        "    feature_mean = mean(values)\n",
        "    feature_median = median(values)\n",
        "    feature_min = min(values)\n",
        "    feature_max = max(values)\n",
        "    feature_std = pstdev(values) if len(values) > 1 else 0.0\n",
        "    print(f\"- {feature:<15} mean={feature_mean:6.2f} | median={feature_median:6.2f} | min={feature_min:6.2f} | max={feature_max:6.2f} | std={feature_std:6.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Categorical distributions:\n",
            "\n",
            "Genre counts:\n",
            "  - Pop          5\n",
            "  - Electronic   5\n",
            "  - Rock         3\n",
            "  - Folk         2\n",
            "  - Alternative  2\n",
            "  - Indie        2\n",
            "  - World        1\n",
            "  - Hip-Hop      1\n",
            "  - Soul         1\n",
            "  - Chillwave    1\n",
            "  - Lo-Fi        1\n",
            "  - Ambient      1\n",
            "\n",
            "Mood counts:\n",
            "  - Energetic    9\n",
            "  - Calm         8\n",
            "  - Happy        5\n",
            "  - Sad          3\n",
            "\n",
            "Era counts:\n",
            "  - 2010s        9\n",
            "  - 2000s        6\n",
            "  - 2020s        5\n",
            "  - 1990s        2\n",
            "  - 1980s        2\n",
            "  - 1970s        1\n"
          ]
        }
      ],
      "source": [
        "print(\"Categorical distributions:\")\n",
        "for field in CATEGORICAL_FIELDS:\n",
        "    counts = Counter(categorical_data[field])\n",
        "    print(f\"\\n{field.title()} counts:\")\n",
        "    for value, count in counts.most_common():\n",
        "        print(f\"  - {value:<12} {count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No missing values detected in the sample dataset.\n"
          ]
        }
      ],
      "source": [
        "def missing_value_counts(rows, column_names):\n",
        "    totals = {name: 0 for name in column_names}\n",
        "    for row in rows:\n",
        "        for name in column_names:\n",
        "            value = row.get(name)\n",
        "            if value is None or value == '':\n",
        "                totals[name] += 1\n",
        "    return totals\n",
        "\n",
        "missing_counts = missing_value_counts(dataset, columns)\n",
        "if any(missing_counts.values()):\n",
        "    print(\"Missing values detected:\")\n",
        "    for name, count in missing_counts.items():\n",
        "        if count:\n",
        "            print(f\"- {name}: {count}\")\n",
        "else:\n",
        "    print(\"No missing values detected in the sample dataset.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Exploratory insights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average energy by genre (0-1 scale):\n",
            "  Alternative  ███████████████  0.54\n",
            "  Ambient      █████████████  0.47\n",
            "  Chillwave    ████████████  0.44\n",
            "  Electronic   ███████████████████████  0.82\n",
            "  Folk         ██████████  0.36\n",
            "  Hip-Hop      █████████████████████████  0.91\n",
            "  Indie        ██████████████  0.50\n",
            "  Lo-Fi        ███████████  0.41\n",
            "  Pop          ████████████████████  0.73\n",
            "  Rock         ██████████████████████  0.78\n",
            "  Soul         ████████████  0.43\n",
            "  World        █████████████  0.45\n",
            "\n",
            "Average danceability by era (0-1 scale):\n",
            "  1970s    ███████████  0.40\n",
            "  1980s    ████████████  0.44\n",
            "  1990s    ██████████████  0.49\n",
            "  2000s    ████████████████  0.59\n",
            "  2010s    ███████████████████  0.67\n",
            "  2020s    █████████████████████  0.74\n"
          ]
        }
      ],
      "source": [
        "def ascii_bar(value, scale=28, symbol='█'):\n",
        "    filled = max(1, int(round(value * scale)))\n",
        "    return symbol * filled\n",
        "\n",
        "genre_groups = defaultdict(list)\n",
        "for row in dataset:\n",
        "    genre_groups[row['genre']].append(row)\n",
        "\n",
        "print(\"Average energy by genre (0-1 scale):\")\n",
        "for genre in sorted(genre_groups):\n",
        "    avg_energy = mean(item['energy'] for item in genre_groups[genre])\n",
        "    print(f\"  {genre:<12} {ascii_bar(avg_energy)} {avg_energy:5.2f}\")\n",
        "\n",
        "era_groups = defaultdict(list)\n",
        "for row in dataset:\n",
        "    era_groups[row['era']].append(row)\n",
        "\n",
        "print(\"\\nAverage danceability by era (0-1 scale):\")\n",
        "for era in sorted(era_groups):\n",
        "    avg_danceability = mean(item['danceability'] for item in era_groups[era])\n",
        "    print(f\"  {era:<8} {ascii_bar(avg_danceability)} {avg_danceability:5.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pearson correlation matrix for selected features:\n",
            "                  danceabili      energy     valence  popularity\n",
            "      danceabili        1.00        0.89        0.85        0.93\n",
            "          energy        0.89        1.00        0.67        0.95\n",
            "         valence        0.85        0.67        1.00        0.81\n",
            "      popularity        0.93        0.95        0.81        1.00\n"
          ]
        }
      ],
      "source": [
        "def pearson_r(x_values, y_values):\n",
        "    mean_x = mean(x_values)\n",
        "    mean_y = mean(y_values)\n",
        "    num = sum((x - mean_x) * (y - mean_y) for x, y in zip(x_values, y_values))\n",
        "    denom_x = math.sqrt(sum((x - mean_x) ** 2 for x in x_values))\n",
        "    denom_y = math.sqrt(sum((y - mean_y) ** 2 for y in y_values))\n",
        "    if denom_x == 0 or denom_y == 0:\n",
        "        return 0.0\n",
        "    return num / (denom_x * denom_y)\n",
        "\n",
        "core_features = ['danceability', 'energy', 'valence', 'popularity']\n",
        "header = ' ' * 16 + ''.join(f\"{name[:10]:>12}\" for name in core_features)\n",
        "print('Pearson correlation matrix for selected features:')\n",
        "print(header)\n",
        "for fx in core_features:\n",
        "    row_text = f\"{fx[:10]:>16}\"\n",
        "    for fy in core_features:\n",
        "        corr = 1.0 if fx == fy else pearson_r(numeric_data[fx], numeric_data[fy])\n",
        "        row_text += f\"{corr:>12.2f}\"\n",
        "    print(row_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Baseline genre model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature standardization reference:\n",
            "  tempo           mean=113.20 | std= 18.39\n",
            "  danceability    mean=  0.62 | std=  0.12\n",
            "  energy          mean=  0.64 | std=  0.18\n",
            "  valence         mean=  0.54 | std=  0.13\n",
            "  loudness        mean= -6.46 | std=  1.94\n",
            "  acousticness    mean=  0.29 | std=  0.19\n",
            "  instrumentalness mean=  0.16 | std=  0.13\n",
            "  speechiness     mean=  0.05 | std=  0.01\n",
            "  liveness        mean=  0.16 | std=  0.05\n",
            "  popularity      mean= 62.32 | std=  9.84\n"
          ]
        }
      ],
      "source": [
        "def standardize_features(rows, features):\n",
        "    means = {feature: mean(numeric_data[feature]) for feature in features}\n",
        "    stds = {feature: pstdev(numeric_data[feature]) or 1.0 for feature in features}\n",
        "    matrix = []\n",
        "    for row in rows:\n",
        "        matrix.append([(row[feature] - means[feature]) / (stds[feature] if stds[feature] else 1.0) for feature in features])\n",
        "    return matrix, means, stds\n",
        "\n",
        "feature_matrix, feature_means, feature_stds = standardize_features(dataset, NUMERIC_FEATURES)\n",
        "print(\"Feature standardization reference:\")\n",
        "for feature in NUMERIC_FEATURES:\n",
        "    print(f\"  {feature:<15} mean={feature_means[feature]:6.2f} | std={feature_stds[feature]:6.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SoftmaxRegression:\n",
        "    def __init__(self, n_features, n_classes, learning_rate=0.25):\n",
        "        self.n_features = n_features\n",
        "        self.n_classes = n_classes\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weights = [[0.0 for _ in range(n_features + 1)] for _ in range(n_classes)]\n",
        "\n",
        "    def _logits(self, features):\n",
        "        extended = features + [1.0]\n",
        "        return [sum(self.weights[class_idx][j] * extended[j] for j in range(self.n_features + 1)) for class_idx in range(self.n_classes)]\n",
        "\n",
        "    def predict_proba(self, features):\n",
        "        logits = self._logits(features)\n",
        "        max_logit = max(logits)\n",
        "        exps = [math.exp(value - max_logit) for value in logits]\n",
        "        total = sum(exps)\n",
        "        return [value / total for value in exps]\n",
        "\n",
        "    def predict(self, features):\n",
        "        logits = self._logits(features)\n",
        "        return max(range(self.n_classes), key=lambda idx: logits[idx])\n",
        "\n",
        "    def fit(self, samples, targets, epochs=1800, report_every=450):\n",
        "        for epoch in range(epochs):\n",
        "            grads = [[0.0 for _ in range(self.n_features + 1)] for _ in range(self.n_classes)]\n",
        "            loss_total = 0.0\n",
        "            for features, target in zip(samples, targets):\n",
        "                extended = features + [1.0]\n",
        "                logits = [sum(self.weights[class_idx][j] * extended[j] for j in range(self.n_features + 1)) for class_idx in range(self.n_classes)]\n",
        "                max_logit = max(logits)\n",
        "                exps = [math.exp(value - max_logit) for value in logits]\n",
        "                total = sum(exps)\n",
        "                probs = [value / total for value in exps]\n",
        "                loss_total += -math.log(max(probs[target], 1e-12))\n",
        "                for class_idx in range(self.n_classes):\n",
        "                    diff = probs[class_idx] - (1 if class_idx == target else 0)\n",
        "                    for j in range(self.n_features + 1):\n",
        "                        grads[class_idx][j] += diff * extended[j]\n",
        "            sample_count = len(samples)\n",
        "            for class_idx in range(self.n_classes):\n",
        "                for j in range(self.n_features + 1):\n",
        "                    self.weights[class_idx][j] -= self.learning_rate * grads[class_idx][j] / sample_count\n",
        "            if report_every and (epoch + 1) % report_every == 0:\n",
        "                avg_loss = loss_total / sample_count\n",
        "                print(f\"Epoch {epoch + 1:4d}: average cross-entropy loss = {avg_loss:.4f}\")\n",
        "\n",
        "def accuracy_score(true_labels, predicted_labels):\n",
        "    correct = sum(1 for true, pred in zip(true_labels, predicted_labels) if true == pred)\n",
        "    return correct / len(true_labels)\n",
        "\n",
        "def classification_report(label_names, true_labels, predicted_labels):\n",
        "    lines = [f\"{'Class':<15}{'Precision':>10}{'Recall':>10}{'F1':>10}{'Support':>10}\"]\n",
        "    macro_precision = macro_recall = macro_f1 = 0.0\n",
        "    for idx, name in enumerate(label_names):\n",
        "        tp = sum(1 for true, pred in zip(true_labels, predicted_labels) if true == idx and pred == idx)\n",
        "        fp = sum(1 for true, pred in zip(true_labels, predicted_labels) if true != idx and pred == idx)\n",
        "        fn = sum(1 for true, pred in zip(true_labels, predicted_labels) if true == idx and pred != idx)\n",
        "        support = sum(1 for true in true_labels if true == idx)\n",
        "        precision = tp / (tp + fp) if (tp + fp) else 0.0\n",
        "        recall = tp / (tp + fn) if (tp + fn) else 0.0\n",
        "        f1 = 2 * precision * recall / (precision + recall) if (precision and recall) else 0.0\n",
        "        macro_precision += precision\n",
        "        macro_recall += recall\n",
        "        macro_f1 += f1\n",
        "        lines.append(f\"{name:<15}{precision:>10.2f}{recall:>10.2f}{f1:>10.2f}{support:>10}\")\n",
        "    label_count = len(label_names)\n",
        "    lines.append('-' * 55)\n",
        "    lines.append(f\"{'Macro avg':<15}{(macro_precision / label_count):>10.2f}{(macro_recall / label_count):>10.2f}{(macro_f1 / label_count):>10.2f}{len(true_labels):>10}\")\n",
        "    return '\\n'.join(lines)\n",
        "\n",
        "def confusion_matrix_str(label_names, true_labels, predicted_labels):\n",
        "    header = ' ' * 12 + ''.join(f\"{name[:9]:>10}\" for name in label_names)\n",
        "    lines = [header]\n",
        "    for idx, name in enumerate(label_names):\n",
        "        row_counts = [0 for _ in label_names]\n",
        "        for true, pred in zip(true_labels, predicted_labels):\n",
        "            if true == idx:\n",
        "                row_counts[pred] += 1\n",
        "        row = f\"{name[:9]:>12}\" + ''.join(f\"{count:>10}\" for count in row_counts)\n",
        "        lines.append(row)\n",
        "    return '\\n'.join(lines)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  450: average cross-entropy loss = 0.2484\n",
            "Epoch  900: average cross-entropy loss = 0.1306\n",
            "Epoch 1350: average cross-entropy loss = 0.0878\n",
            "Epoch 1800: average cross-entropy loss = 0.0659\n",
            "Training accuracy: 1.00\n",
            "Test accuracy: 0.57\n",
            "\n",
            "Classification report (test set):\n",
            "Class           Precision    Recall        F1   Support\n",
            "Alternative          1.00      1.00      1.00         1\n",
            "Ambient              0.00      0.00      0.00         0\n",
            "Chillwave            0.00      0.00      0.00         0\n",
            "Electronic           1.00      1.00      1.00         2\n",
            "Folk                 0.00      0.00      0.00         0\n",
            "Hip-Hop              0.00      0.00      0.00         0\n",
            "Indie                0.00      0.00      0.00         0\n",
            "Lo-Fi                0.00      0.00      0.00         1\n",
            "Pop                  1.00      0.50      0.67         2\n",
            "Rock                 0.00      0.00      0.00         0\n",
            "Soul                 0.00      0.00      0.00         0\n",
            "World                0.00      0.00      0.00         1\n",
            "-------------------------------------------------------\n",
            "Macro avg            0.25      0.21      0.22         7\n",
            "\n",
            "Confusion matrix (test set):\n",
            "             Alternati   Ambient Chillwave Electroni      Folk   Hip-Hop     Indie     Lo-Fi       Pop      Rock      Soul     World\n",
            "   Alternati         1         0         0         0         0         0         0         0         0         0         0         0\n",
            "     Ambient         0         0         0         0         0         0         0         0         0         0         0         0\n",
            "   Chillwave         0         0         0         0         0         0         0         0         0         0         0         0\n",
            "   Electroni         0         0         0         2         0         0         0         0         0         0         0         0\n",
            "        Folk         0         0         0         0         0         0         0         0         0         0         0         0\n",
            "     Hip-Hop         0         0         0         0         0         0         0         0         0         0         0         0\n",
            "       Indie         0         0         0         0         0         0         0         0         0         0         0         0\n",
            "       Lo-Fi         0         0         0         0         1         0         0         0         0         0         0         0\n",
            "         Pop         0         0         0         0         0         0         0         0         1         1         0         0\n",
            "        Rock         0         0         0         0         0         0         0         0         0         0         0         0\n",
            "        Soul         0         0         0         0         0         0         0         0         0         0         0         0\n",
            "       World         0         0         0         0         0         0         0         0         0         0         1         0\n",
            "\n",
            "Sample predictions from the test set:\n",
            "  - Desert Echoes (World) -> predicted Soul\n",
            "  - Crystalline (Electronic) -> predicted Electronic\n",
            "  - Silent Street (Alternative) -> predicted Alternative\n",
            "  - Voltage Bloom (Electronic) -> predicted Electronic\n",
            "  - Neon Skies (Pop) -> predicted Rock\n"
          ]
        }
      ],
      "source": [
        "labels = sorted({row['genre'] for row in dataset})\n",
        "label_to_index = {label: idx for idx, label in enumerate(labels)}\n",
        "index_to_label = {idx: label for label, idx in label_to_index.items()}\n",
        "\n",
        "indices = list(range(len(dataset)))\n",
        "random.shuffle(indices)\n",
        "split = int(len(indices) * 0.75)\n",
        "train_indices = indices[:split]\n",
        "test_indices = indices[split:]\n",
        "\n",
        "X_train = [feature_matrix[idx] for idx in train_indices]\n",
        "y_train = [label_to_index[dataset[idx]['genre']] for idx in train_indices]\n",
        "X_test = [feature_matrix[idx] for idx in test_indices]\n",
        "y_test = [label_to_index[dataset[idx]['genre']] for idx in test_indices]\n",
        "\n",
        "model = SoftmaxRegression(n_features=len(NUMERIC_FEATURES), n_classes=len(labels), learning_rate=0.25)\n",
        "model.fit(X_train, y_train, epochs=1800, report_every=450)\n",
        "\n",
        "train_predictions = [model.predict(features) for features in X_train]\n",
        "test_predictions = [model.predict(features) for features in X_test]\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_predictions)\n",
        "test_accuracy = accuracy_score(y_test, test_predictions)\n",
        "\n",
        "print(f\"Training accuracy: {train_accuracy:.2f}\")\n",
        "print(f\"Test accuracy: {test_accuracy:.2f}\")\n",
        "\n",
        "ordered_labels = [index_to_label[idx] for idx in range(len(labels))]\n",
        "print(\"\\nClassification report (test set):\")\n",
        "print(classification_report(ordered_labels, y_test, test_predictions))\n",
        "\n",
        "print(\"\\nConfusion matrix (test set):\")\n",
        "print(confusion_matrix_str(ordered_labels, y_test, test_predictions))\n",
        "\n",
        "print(\"\\nSample predictions from the test set:\")\n",
        "for idx in test_indices[:5]:\n",
        "    predicted_label = ordered_labels[model.predict(feature_matrix[idx])]\n",
        "    true_label = dataset[idx]['genre']\n",
        "    print(f\"  - {dataset[idx]['track_name']} ({true_label}) -> predicted {predicted_label}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Next steps\n",
        "\n",
        "* Expand the dataset by connecting to the Spotify Web API and Billboard or Top 500 chart archives.\n",
        "* Engineer temporal and popularity-alignment features that capture whether a track matches current trends.\n",
        "* Compare additional classifiers (Random Forest, Gradient Boosting, LightGBM) against the custom baseline.\n",
        "* Begin prototyping the Streamlit interface for interactive visualizations and explanations.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
